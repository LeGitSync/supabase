---
title: 'Configure S3 Storage'
description: 'Set up an S3 backend and enable the S3-compatible client endpoint for self-hosted Supabase Storage.'
subtitle: 'Set up an S3 backend and enable the S3-compatible client endpoint for self-hosted Supabase Storage.'
---

Self-hosted Supabase Storage has two independent S3-related features:

- **S3 backend** — where Storage physically stores files. By default, files are stored on the local filesystem. You can switch to an S3-compatible service (AWS S3, MinIO, Cloudflare R2, etc.) for durability, scalability, or to use existing infrastructure.
- **S3 protocol endpoint** — an S3-compatible API that Storage exposes at `/storage/v1/s3`. This allows standard S3 tools like `rclone` and the AWS CLI to interact with your Storage instance. It works with both the file backend and the S3 backend.

You can configure either feature independently.

## Configure an S3 backend

To switch Storage from the default file backend to an S3 backend, update the `storage` service environment in your `docker-compose.yml`:

```yaml docker-compose.yml
storage:
  environment:
    # ... existing variables ...
    STORAGE_BACKEND: s3
    GLOBAL_S3_BUCKET: your-bucket-name
    GLOBAL_S3_ENDPOINT: https://your-s3-endpoint
    GLOBAL_S3_FORCE_PATH_STYLE: "true"
    GLOBAL_S3_PROTOCOL: https
    AWS_ACCESS_KEY_ID: your-access-key
    AWS_SECRET_ACCESS_KEY: your-secret-key
    REGION: your-region
```

Remove or leave the existing `FILE_STORAGE_BACKEND_PATH` — it is ignored when `STORAGE_BACKEND` is set to `s3`.

Restart the storage service to apply changes:

```bash
docker compose restart storage --no-deps
```

The following sections show provider-specific configuration.

### MinIO (local S3-compatible storage)

MinIO runs alongside your self-hosted instance and provides an S3-compatible API locally. Add the MinIO service and an init container to your `docker-compose.yml`:

```yaml docker-compose.yml
services:
  # ... existing services ...

  minio:
    image: minio/minio
    ports:
      - '9000:9000'
      - '9001:9001'
    environment:
      MINIO_ROOT_USER: your-minio-user
      MINIO_ROOT_PASSWORD: your-minio-password
    command: server --console-address ":9001" /data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://minio:9000/minio/health/live"]
      interval: 2s
      timeout: 10s
      retries: 5
    volumes:
      - ./volumes/storage:/data:z

  minio-createbucket:
    image: minio/mc
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set supa-minio http://minio:9000 your-minio-user your-minio-password;
      /usr/bin/mc mb --ignore-existing supa-minio/your-bucket-name;
      exit 0;
      "
```

Then configure the storage service:

```yaml docker-compose.yml
storage:
  depends_on:
    # ... existing dependencies ...
    minio:
      condition: service_healthy
  environment:
    # ... existing variables ...
    STORAGE_BACKEND: s3
    GLOBAL_S3_BUCKET: your-bucket-name
    GLOBAL_S3_ENDPOINT: http://minio:9000
    GLOBAL_S3_PROTOCOL: http
    GLOBAL_S3_FORCE_PATH_STYLE: "true"
    AWS_ACCESS_KEY_ID: your-minio-user
    AWS_SECRET_ACCESS_KEY: your-minio-password
    REGION: us-east-1
```

The MinIO console is available at `http://localhost:9001`.

<Admonition type="tip">

A preconfigured `docker-compose.s3.yml` overlay exists in the [self-hosted repository](https://github.com/supabase/supabase/blob/master/docker/docker-compose.s3.yml). You can use it as a reference, but it may not always reflect the latest storage image version.

</Admonition>

### AWS S3

Create an S3 bucket and an IAM user with access to it. Then configure the storage service:

```yaml docker-compose.yml
storage:
  environment:
    # ... existing variables ...
    STORAGE_BACKEND: s3
    GLOBAL_S3_BUCKET: your-aws-bucket-name
    REGION: us-east-1
    AWS_ACCESS_KEY_ID: your-aws-access-key
    AWS_SECRET_ACCESS_KEY: your-aws-secret-key
```

For AWS S3, you do not need `GLOBAL_S3_ENDPOINT` or `GLOBAL_S3_FORCE_PATH_STYLE` — the defaults work with standard AWS endpoints.

### S3-compatible providers (Cloudflare R2, Hetzner, DigitalOcean Spaces)

Use the same configuration as MinIO, but point to your provider's endpoint:

```yaml docker-compose.yml
storage:
  environment:
    # ... existing variables ...
    STORAGE_BACKEND: s3
    GLOBAL_S3_BUCKET: your-bucket-name
    GLOBAL_S3_ENDPOINT: https://your-account-id.r2.cloudflarestorage.com
    GLOBAL_S3_PROTOCOL: https
    GLOBAL_S3_FORCE_PATH_STYLE: "true"
    AWS_ACCESS_KEY_ID: your-provider-access-key
    AWS_SECRET_ACCESS_KEY: your-provider-secret-key
    REGION: auto
```

<Admonition type="caution">

**Cloudflare R2** does not support the `x-amz-tagging` header. If you encounter TUS upload errors (HTTP 500 with `Header 'x-amz-tagging' not implemented`), add this to the storage service environment:

```yaml
TUS_ALLOW_S3_TAGS: "false"
```

This may also apply to other S3-compatible providers that don't fully implement the AWS S3 tagging API.

</Admonition>

## Enable the S3 protocol endpoint

The S3 protocol endpoint at `/storage/v1/s3` lets standard S3 clients interact with your Storage instance. By default, this endpoint rejects all requests because no credentials are configured. The Supabase REST API and SDK are not affected — only the S3 protocol endpoint requires these credentials.

Generate a key pair:

```bash
openssl rand -hex 16   # use as S3_PROTOCOL_ACCESS_KEY_ID
openssl rand -hex 32   # use as S3_PROTOCOL_ACCESS_KEY_SECRET
```

Add both to the `storage` service environment in `docker-compose.yml`:

```yaml docker-compose.yml
storage:
  environment:
    # ... existing variables ...
    S3_PROTOCOL_ACCESS_KEY_ID: your-generated-access-key-id
    S3_PROTOCOL_ACCESS_KEY_SECRET: your-generated-access-key-secret
```

Restart the storage service:

```bash
docker compose restart storage --no-deps
```

### Test with the AWS CLI

```bash
aws s3 ls \
  --endpoint-url http://localhost:8000/storage/v1/s3 \
  --region us-east-1 \
  s3://your-bucket-name \
  --no-sign-request=false
```

Use the `S3_PROTOCOL_ACCESS_KEY_ID` and `S3_PROTOCOL_ACCESS_KEY_SECRET` values when prompted, or configure them in `~/.aws/credentials`:

```ini ~/.aws/credentials
[selfhosted]
aws_access_key_id = your-generated-access-key-id
aws_secret_access_key = your-generated-access-key-secret
```

```bash
aws s3 ls \
  --endpoint-url http://localhost:8000/storage/v1/s3 \
  --region us-east-1 \
  --profile selfhosted \
  s3://
```

## Verify

1. Open Studio and upload a file to a bucket.
2. If using an S3 backend: confirm the file appears in your S3 provider's console (MinIO at `http://localhost:9001`, AWS Console, R2 Dashboard, etc.).
3. If you enabled the S3 protocol: list the file using the AWS CLI or rclone to confirm the endpoint works.

## Troubleshooting

### Signature mismatch errors

S3 clients sign requests using the access key ID and secret. If you see `SignatureDoesNotMatch`, verify that the `S3_PROTOCOL_ACCESS_KEY_ID` and `S3_PROTOCOL_ACCESS_KEY_SECRET` in your `docker-compose.yml` exactly match what your S3 client is using. Restart storage after any changes.

### TUS upload errors on Cloudflare R2

If resumable (TUS) uploads fail with HTTP 500 and a message about `x-amz-tagging`, add `TUS_ALLOW_S3_TAGS: "false"` to the storage service environment. Cloudflare R2 does not implement this S3 feature.

### Permission denied on uploads

Setting a bucket to "Public" only allows unauthenticated **downloads**. Uploads are always blocked unless you create an RLS policy on the `storage.objects` table. Go to Storage > Policies in Studio and create a policy that allows `INSERT` for the appropriate roles.

### Upload URLs point to localhost

If uploads from a browser fail (CORS or mixed content errors), check that `API_EXTERNAL_URL` and `SUPABASE_PUBLIC_URL` in your `.env` file match your actual domain and protocol — not `http://localhost:8000`.

### Additional resources

- [Storage repository `.env.sample`](https://github.com/supabase/storage/blob/master/.env.sample) — full list of environment variables
- [S3 Authentication](/docs/guides/storage/s3/authentication) — S3 credential types and session tokens
